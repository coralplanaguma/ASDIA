---
title: "ASDIA Projet 1"
output:
  html_document:
    df_print: paged
---



# Partie 1 : Apprentissage supervisé pour la classification 

L'objet de cette partie est de démontrer le résultat suivant.

**Théorème.** *Soit $q({\bf x})$ une fonction arbitraire à valeur dans $[0,1]$ et* 
$$
q_{\rm opt}({\bf x}) = p(y = 1|{\bf x}) \, , \quad \forall {\bf x}.
$$
*Alors, nous avons*
$$
\mathbb{E}[L(q({\bf x}), y )] \geq \mathbb{E}[L(q_{\rm opt}({\bf x}), y )] .
$$

*  **Conditionnement** : 

$$
\mathbb{E}[L(q({\bf x}), y)]=\int\mathbb{E}[L(q({\bf x}), y)| x]p({\bf x})dx
$$
Comme $p({\bf x }) \in [0, 1] (>0)$, pour minimiser l'espérance $\mathbb{E}[L(q({\bf x}), y )]$ il suffit de minimiser l'espérance conditionnelle $\mathbb{E}[L(q({\bf x}), y ) | {\bf x}]$ pour tout ${\bf x}$.

*  **Entropie croisée** :

$$
\begin{align}
\mathbb{KL}(p \| p^\prime) &= p(y = 0) \log \frac{p(y = 0)}{p \prime y = 0} + p(y =1) \log \frac{p(y=1)}{p \prime (y = 1)}\\
&= h(p, p\prime) + p(y=0)\log p(y=0) + p(y=1) \log p(y=1)\\
&= h(p, p \prime) - h(p)
\end{align}
$$

Comme la divergence de Kulback-Leibler est positive, on obtient bien l'inégalité suivante : 
$$
h(p, p') \geq h(p)
$$


* **Entropie croisée et log-loss ** : On appelle $q_{.}({\bf x})$ la loi de Bernoulli de paramètre $q({\bf x})$. Démontrer que 

$$
\mathbb{E}[L(q({\bf x}), y ) | {\bf x}]  = h(p_{\sf Y}(.|{\bf x}), q_{.}({\bf x}))  .  
$$


* Démontrer le théorème. 


En conclusion, minimiser l'espérance conditionnelle $\mathbb{E}[L(q({\bf x}), y) | {\bf x} ]$ revient à minimiser la divergence $D_{KL}(  p_{\sf y|{\bf x}} \| q({\bf x}) )$. Ainsi, nous pouvons affirmer que minimiser la perte moyenne log-loss revient à minimiser l'information perdue lorsque l'on approche la loi cible $p(y|{\bf x})$ par un modèle probabiliste $q({\bf x})$. 


* On suppose que la perte moyenne est évaluée à partir d'un échantillon de données d'apprentissage $({\bf x}_i, y_i)$, $i = 1, \dots, n$, indépendantes et de même loi. On suppose que la loi conditionnelle est modélisée à l'aide d'ensemble de paramètres, $\theta$, par la fonction suivante

$$
p(y = 1 | {\bf x}, \theta) = q({\bf x}, \theta) \, .
$$
Démontrer que le minimum de la fonction de perte correspond au maximum de la vraisemblance pour ce modèle. 


# Partie 2 : Reconnaissance de chiffres manuscrits (2 et 7)

## Lecture des données

* Lire les données MNIST. Il y a 4 ensembles de variables. 
```{r}
  library(keras)
  library(magrittr)
  mnist <- dataset_mnist()
  x_train <- mnist$train$x
  y_train <- mnist$train$y
  x_test <- mnist$test$x
  y_test <- mnist$test$y
```

* Filtrer les données correspondant aux chiffres $2$ et $7$. Commenter et compléter le code suivant. Exécuter le code en changeant l'option de chunck. 

```{r eval = FALSE}
# On conserve les 2 et les 7 uniquement
  boo_train <- y_train == 2 | y_train == 7 
  x_train <- mnist$train$x[boo_train,,]
  y_train <- mnist$train$y[boo_train]

# Idem pour le set de test
  boo_test <- y_test == 2 | y_test == 7
  x_test <- mnist$test$x[boo_test,,]
  y_test <- mnist$test$y[boo_test]
```


* Utiliser la fonction `image()` pour visualiser le premier chiffre test de la base de données réduite. C'est un 7. 

```{r}
  image(t(x_test[1, 28:1,]), col = grey.colors(5))
```


* Les images de dimension 28x28 doivent être converties en vecteurs de longueur 784 ($= 28 \times 28$). Cela peut se faire de plusieurs manières. En particulier, la fonction `array_reshape()` de keras est très utile pour cela.  


```{r eval = FALSE}
# reshape
  x_train <- array_reshape(x_train, c(nrow(x_train), 784))
  x_test <- array_reshape(x_test, c(nrow(x_test), 784))
```

* Normaliser les données pour obtenir des valeurs réelles (flottants) entre 0 et 1 en divisant les valeurs présentes par 255.  Utiliser la fonction `image()` pour visualiser le premier chiffre test de la base de données réduite dans cette nouvelle représentation. 


```{r eval = FALSE}
# rescale
  x_train <- x_train/255
  x_test <- x_test/255

# le symbol %>% est similaire au 'pipe' d'unix (library(magrittr))
  dim(x_test)
  x_test[1,] %>% matrix(nrow = 28) %>% .[,28:1] %>% image(col = grey.colors(5))
```


* Les données de classe sont des entiers 2,7. Convertir ces données en variables booléennes ou binaires. La valeur 1 ou `TRUE` correspondra à un 7.
  
```{r eval = FALSE}
  y_train <- y_train == 7
  y_test <-  y_test == 7
```


## Ajuster un réseau de neurone avec keras

* Construire un réseau de neurones à deux couches cachées à l'aide de la fonction `keras_model_sequential()`, en faisant varier les paramètres des couches comme ci-dessous. Définir les termes apparaissant dans la construction du modèle.


* Commenter et compléter le code suivant.  

```{r eval = FALSE}
  model <- keras_model_sequential() 
  model %>% 
    # Initialisation de la première couche avec fonctoin d'activation "relu" et 256 neurones.
    layer_dense(units = 256, activation = 'relu', input_shape = 784) %>% 
    # On associe aléatoirement la valeur 0 à 50% des imahes d'entrée pour éviter l'over fitting.
    layer_dropout(rate = 0.5) %>% 
    # Initialisation de la deuxième couche avec 128 neurones.
    layer_dense(units = 128, activation = 'relu') %>%
    layer_dropout(rate = 0.5) %>%
    # Initialisation de dernière couche
    layer_dense(units = 1, activation = 'sigmoid')
```

* Donner un tableau de correspondance entre les notions mathématiques introduites dans la section précédente et les termes apparaissant dans la construction du modèle. 

```{r}
terme <- c("units = 256", "input_shape")
definition <- c("Dimension de la première couche cachée (W_2)", "Dimension des observations")
data.frame(terme, definition)
```


* Compiler le modèle en précisant la fonction de perte (binary) et demander de visualiser le taux de bonne classification (accuracy). Le choix de l'optimiseur est spécifié. 


```{r eval = FALSE}
# Configuration du modèle pour l'entraînement  
  model %>% compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_rmsprop(lr = 0.001, rho = 0.9, decay = 0),
    metrics = c('accuracy')
)
```

* Donner un tableau de correspondance entre les notions mathématiques introduites dans la section précédente et les termes apparaissant dans la compilation du modèle. 

```{r}
terme <- c("loss", "optimizer", "lr", "rho", "decay", "accuracy")
definition <- c("Fonction de perte","Méthode de gradient stochastique","Taux d'apprentissage (pas dans la descente de gradient)","Facteur de décroissance moyenne du gradient","Baisse du taux d'apprentissage à chaque itération","Précision utilisée pour mesurer la pertinence du modèle")
data.frame(terme, definition)
```



* Allons-y Alonso, pour ajuster le réseau (apprentissage)

```{r, eval = FALSE}
# A pprentissage : on entraîne le modèle
  history <- model %>% fit(
                        x_train, 
                        y_train, 
                        epochs = 20, 
                        batch_size = 128, 
                        validation_data = list(x_test, y_test)
)
```

* Donner un tableau de correspondance entre les notions mathématiques introduites dans la section précédente et les termes apparaissant dans l'apprentissage du modèle. 
```{r}
terme <- c("x_train", "y_train", "epochs", "batch_size", "validation_data")
definition <- c("Ensemble des images (observations)","Sortie attendue","Nombre d'images utilisé par le réseau à chaque itération","Nombre d'entraînements du modèle", "Données permettant d'évaluer le modèle")
data.frame(terme, definition)
```
* Quelle est signification des courbes `loss` et `val_loss` que l'on voit tracées? 

```{r eval = FALSE}
plot(history)
```
Les coubes "loss" et "val_loss" représentent respectivement les pertes du modèle pour les données d'entraînement (x_train, y_train) et pour les données de tests (x_test, y_test).

* Evaluer le modèle sur les données de test (erreur de classification et perte log loss)


```{r eval = FALSE}
model %>% evaluate(x_test, y_test)
```


* Donner une matrice de confusion pour les classements effectués par le modèle sur le jeu test

```{r eval = FALSE}
# help(table)
pred_class <- model %>% predict_classes(x_test)
table(predicted = pred_class, observed = mnist$test$y[boo_test])
```

* Montrer deux chiffres manuscrits que la machine n'a pas réussi à classer correctement. Donner la liste des mal classés et les probabilités de classement pour chacune des erreurs.

```{r eval = FALSE}
index = 1:2060
wrong = index[pred_class[, 1]!=y_test]

x_test[145,] %>% matrix(nrow = 28) %>% .[,28:1] %>% image(col = grey.colors(5))
x_test[223,] %>% matrix(nrow = 28) %>% .[,28:1] %>% image(col = grey.colors(5))


predict_proba(model, x_test)[145]
predict_proba(model, x_test)[223]

```


## Défi MNIST 

Pour les données de l'exercice précédent, répondre aux questions suivantes.

* Reporter dans un tableau 6x2 (`data.frame`), puis dans deux diagrammes en barres (`barplot`), les valeurs des erreurs de classification et de _logloss_ obtenues sur l'ensemble test pour 6 réseaux de neurones distincts dont on aura fait varier les paramètres de la manière suivante 
  - nombre de couches cachées : 1 ou 5, 
  - nombre de neurones par couche cachée : 10 ou 100, 
  - valeur de `dropout` par couche cachée : 0.2 ou 0.8.
  
```{r}
nb_couches <-c(1, 5)
nb_neurones<-c(10, 100)
val_dropout<-c(0.2, 0.8)

res_log_loss <- c()
res_acc<-c()
names<-c()

create_model = function (nb_couches, nb_neurones, val_dropout){
  #Renvoie un modèle correspondant aux critères entrés en paramètres
  model <- keras_model_sequential()
  model %>% 
    # Initialisation de la première couche avec fonction d'activation "relu" et 256 neurones.
    layer_dense(units = nb_neurones, activation = 'relu', input_shape = 784) %>% 
    layer_dropout(rate = val_dropout)
  if(nb_couches != 1){
    for (i in 1:nb_couches-1){
      model %>% 
      # Initialisation des couches suivantes
      layer_dense(units = nb_neurones, activation = 'relu') %>% 
      layer_dropout(rate = val_dropout)
    }
  }
  model %>% 
  # Initialisation de dernière couche
  layer_dense(units = 1, activation = 'sigmoid')
  return (model);
}

compteur = 1
for (i in 1:2){
  for (j in 1:2){
    for(k in 1:2){
      model<-create_model(nb_couches[i], nb_neurones[j], val_dropout[k])

      # Configuration du modèle pour l'apprentissage
      model %>% compile(
          loss = 'binary_crossentropy',
          optimizer = optimizer_rmsprop(lr = 0.001, rho = 0.9, decay = 0),
          metrics = c('accuracy')
      )
      
      
      # Entraînement du réseau
      history <- model %>% fit(
                              x_train, 
                              y_train, 
                              epochs = 20, 
                              batch_size = 128, 
                              validation_data = list(x_test, y_test))
      res <- c()
      res = model %>% evaluate(x_test, y_test)
      res_log_loss[compteur] = res$loss
      res_acc[compteur] = res$acc
      names[compteur]=paste(c("modele", nb_couches[i], nb_neurones[j], val_dropout[k]*10), collapse = "_")
      compteur = compteur + 1
  
    }
  }
}

data.frame(names, res_log_loss, res_acc)
barplot(res_log_loss, legend.text = "Diagramme log_loss")
barplot(res_acc, legend.text = "Diagramme des erreurs de classification")

```


* Quel modèle de prédiction vous parait être le meilleur ? 

* Donner la liste des chiffres mal classés et les probabilités de classement pour chacune des erreurs.
# pour ce modèle
```{r}
index = 1:2060
pred_class <- model %>% predict_classes(x_test)
wrong = index[pred_class[, 1]!=y_test]
predict_proba(model, x_test)[145]
```



# Partie 3 : Analyse de critiques de films (IMDB)


```{r}
library(magrittr)
library(keras)

```

## Index des termes et sacs de mots.

```{r}
  index <- keras::dataset_imdb_word_index()
  names(index[index == 1])
```

Sans surprise, le terme le plus fréquemment utilisé est l'article "the". Sa valeur dans l'index est donc égale à 1. 

```{r}
  # Réordonnons l'index
  o <- as.numeric(index) %>% order()
  # Récupération des dix termes les plus utilisés das l'index
  index[o[1:10]] %>% names()
```

Nous voyons qu'il s'agit d'articles, de prépositions ou de termes non-informatifs, comme par exemple, des éléments extraits des balises html (br). Il sera peut-être préférable d'éliminer les termes les plus utilisés. L'entrée 49 correspondant au terme "good".



Pour réduire le temps de calcul, nous conservons uniquement les 5000 termes les plus fréquents. Dans la suite, chaque document sera représenté par une suite de fréquences de termes représentés par les codes de l'index. Le jeu de données résultant de cette étape est nommé `imbd`.

```{r}
# Sékection des données
  imbd <-  keras::dataset_imdb(path = "imdb.npz",
                              num_words = 5048, # nombre de mots conservés
                              skip_top = 49, # élimination des termes les plus utilisés (non pertinents)
                              oov_char = -2, # caractère qui remplace les valeurs qui ont été supprimées
                              start_char = -1, # début de séquence
                              index_from = 0)
```



```{r}
  summary(imbd)
```

## Défi "analyse de sentiments"

### Lecture des données

* Ecrire une ligne de commande R permettant convertir le document 12 en un vecteur de longueur 5000, indiquant le nombre d'apparition de chacun des indices allant de 1 à 5000 dans ce document.  


```{r eval = FALSE}
  # comment 1
  help(sapply)
  result <- sapply(49:5048, FUN = function(x) cogito_ergo_sum)
  result[1:100]
```



* Constituer un jeu de données comportant 10000 documents choisis pour moitié dans l'ensemble "train" et pour moitié dans l'ensemble "test" de l'IMBD. Techniquement nous le constituerons en 20 étapes, pour limiter l'impact sur la mémoire. Commenter et exécuter le code suivant


```{r eval = FALSE}
x_imbd <- NULL

  for (i in 1:10){
    
    x_imbd_500 <- NULL
    
      for (j in (500*(i-1)+1):(500*i)){
        
        # comment 1
        
        doc_temp <- imbd$train$x[[j]]
        x_imbd_500 <- rbind(x_imbd_500, 
                         sapply(49:5048, 
                                FUN = function(ind) sum(doc_temp == ind)))
        
        if (j%%500 == 0) print(j) # ca rassure
    }
    x_imbd <- rbind(x_imbd, x_imbd_500)
  }

 for (i in 1:10){
   
    x_imbd_500 <- NULL
    
      for (j in (500*(i-1)+1):(500*i)){
        
        # comment 2
        
        doc_temp <- imbd$test$x[[j]]
        x_imbd_500 <- rbind(x_imbd_500, 
                         sapply(49:5048, 
                                FUN = function(ind) sum(doc_temp == ind)))
        
        if (j%%500 == 0) print(j) # ca rassure
    }
    x_imbd <- rbind(x_imbd, x_imbd_500)
  }
```

* Que contient l'objet `x_imbd` ?

* Définir les classes $y = 0$ ou $y=1$ pour chaque élément de `x_imbd`

```{r eval = FALSE}
  y_imbd <- c(imbd$train$y[1:5000], imbd$test$y[1:5000])
```

Et voilà. On est en pleine forme et on dispose d'une base d'apprentissage comportant les fréquences d'apparition des mots de l'index pour 10000 documents (`x_imbd`) et les opinions des utilisateurs `y_imbd`. Le défi peut vraiment commencer.


#### Etude d'association

Le but d'une étude d'association est d'identifier les termes les plus associés aux opinions positives ou négatives des utilisateurs. Pour cela, nous evaluons la correlation au carré entre l'occurrence de chaque terme et l'opinion de l'utilisateur (présence d'un 1). Il se peut que certaines valeurs de corrélation ne soient pas calculables à cause d'un écart-type nul. 

* Calculer le coefficient de corrélation au carré entre les fréquences d'apparition des termes de l'index et opinion des utilsateurs (5000 valeurs).

```{r eval = FALSE}
  # comment
  x <- x_imbd
  
  # comment
  y <- y_imbd
  
  # comment
  r <- find_me
  r2 <- r^2
```


* Montrer les termes dont la valeur d'association $r^2$ est supérieure à 3 pour cent (0.02), puis supérieure à 0.02, et à 0.01. _Note_ : Il faut effectuer un décalage de 48 indices dans l'index pour trouver le codage correct.


```{r eval = FALSE}
  # quelque chose à changer
  index[o[ which(r2 > 0.02) + 48 ]] %>% names()
```

* Dans quelles proportions les termes de valeur d'association $r^2$ supérieure à 0.02 apparaissent-ils dans les documents ? Représenter graphiquement ces proportions à l'aide d'un diagramme en barre. 


```{r eval = FALSE}
# Calculer la frequence des termes realisant la condition
  freq <- x[, conditio_logicae ] %>% apply(2, mean) 

# mots dans l'index et barplot
  names(freq) <-  index[o[which(r2 > 0.02) + 48]] %>% names() 
  barplot(sort(freq, decreasing = TRUE), col = "lightblue", las = 2)
```



* Dans quelles proportions les termes de valeur d'association $r^2$ supérieure à 0.02 apparaissent-ils dans les documents ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre.


```{r eval = FALSE}
  yes_we_can
```


* Dans quelles proportions les termes de valeur d'association $r = cor(x,y)$ supérieure à 0.1 apparaissent-ils dans les documents ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre.

```{r eval = FALSE}
  yes_we_can
  barplot(sort(freq, decreasing = TRUE), col = "pink", las = 2)
```



*  Dans quelles proportions les termes de valeur d'association $r$ inférieure à $-0.1$ apparaissent-ils dans les documents ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre.


```{r eval = FALSE}
  i_have_a_dream
  barplot(sort(freq, decreasing = TRUE), col = "palegreen", las = 2)
```



#### Modèles d'apprentissage 


* \`A l'aide des outils de _keras_, ajuster des modèles d'apprentissage aux données contenues dans le défi : "x_imbd" et "y_imbd". Considérer la graine du générateur aléatoire `seed` comme un hyper-paramètre (`set.seed(seed)`).

* Dans un tableau, décrire les performances de 6 méthodes choisies pour des échantillons d'apprentissage et de test que vous aurez créés vous-mêmes à partir des données "x_imbd" et "y_imbd". Les performances seront mesurées par les erreurs de classification et d'entropie croisée (log loss).

* Donner le code R correspondant au meilleur modèle que vous avez ajusté (chunk ci-dessous). On considèrera la graine du générateur aléatoire comme un hyperparamètre supplémentaire du modèle. Donner la valeur finale d'entropie croisée estimée sur votre ensemble test. On fera en sorte que le résulat soit complètement reproductible à partir du code proposé. 


```{r eval = FALSE}
# code de mon meilleur modèle
# Ne pas oublier d'inclure la seed du générateur aléatoire 
# set.seed(seed = 5427381)
  model <-  deus_ex_machina
```




# Règles de rendu

* Ce projet donne lieu à un compte rendu séparé, dans lequel on aura pris soin de reporter les sections de ce document comportant vos réponses, commentaires, et vos codes complétés. Merci de ne pas reporter les sections "théoriques".

* Les codes R doivent être inclus dans le texte du compte-rendu (menu **Insert**) et l'ensemble doit être commenté avec précision. **Les commentaires compteront pour une part importante dans la note**.

* Le compte-rendu doit être déposé **dans TEIDE**. 

* Le compte-rendu doit être déposé **au format HTML uniquement** et intitulé "Rendu_Projet_1.html". Utiliser la fonction **knitr** du menu de rstudio pour obtenir le document au format souhaité. **Les fichiers sources (Rmd ou nb.html) ne seront pas acceptés**.




