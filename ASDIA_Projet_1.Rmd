---
title: "ASDIA Projet 1"
output:
  html_document:
    df_print: paged
---



# Partie 1 : Apprentissage supervisé pour la classification 

L'objet de cette partie est de démontrer le résultat suivant.

**Théorème.** *Soit $q({\bf x})$ une fonction arbitraire à valeur dans $[0,1]$ et* 
$$
q_{\rm opt}({\bf x}) = p(y = 1|{\bf x}) \, , \quad \forall {\bf x}.
$$
*Alors, nous avons*
$$
\mathbb{E}[L(q({\bf x}), y )] \geq \mathbb{E}[L(q_{\rm opt}({\bf x}), y )] .
$$

*  **Conditionnement** : 

$$
\mathbb{E}[L(q({\bf x}), y)]=\int\mathbb{E}[L(q({\bf x}), y)| x]p({\bf x})dx
$$
Comme $p({\bf x }) \in [0, 1] (>0)$, pour minimiser l'espérance $\mathbb{E}[L(q({\bf x}), y )]$ il suffit de minimiser l'espérance conditionnelle $\mathbb{E}[L(q({\bf x}), y ) | {\bf x}]$ pour tout ${\bf x}$.

*  **Entropie croisée** :

$$
\begin{align}
\mathbb{KL}(p \| p^\prime) &= p(y = 0) \log \frac{p(y = 0)}{p \prime y = 0} + p(y =1) \log \frac{p(y=1)}{p \prime (y = 1)}\\
&= h(p, p\prime) + p(y=0)\log p(y=0) + p(y=1) \log p(y=1)\\
&= h(p, p \prime) - h(p)
\end{align}
$$

Comme la divergence de Kulback-Leibler est positive, on obtient bien l'inégalité suivante : 
$$
h(p, p') \geq h(p)
$$


* **Entropie croisée et log-loss ** : On appelle $q_{.}({\bf x})$ la loi de Bernoulli de paramètre $q({\bf x})$. Montrons que: 

$$
\mathbb{E}[L(q({\bf x}), y ) | {\bf x}]  = h(p_{\sf Y}(.|{\bf x}), q_{.}({\bf x}))  .  
$$

Démonstration:

$$
\begin{align}
  \mathbb{E}[L(q({\bf x}, y)|{\bf x})] &= \mathbb{E}[-y \log q({\bf x}) - (1-y)\log (1-q({\bf x}))|{\bf x}]\\
  &= -\log q({\bf x}) \mathbb{E[y]}-\log(1-q({\bf x}))(1 - \mathbb{E}[y])\\
  &=-p(y=1|{\bf x}) \log q({\bf x}) - (1 - p(y=1|{\bf x}))\log (1-q({\bf x}))\\
  &= -p(y=0|{\bf x}) \log (1-q({\bf x})) - p(y=1|{\bf x})\log (q({\bf x}))\\
  &= h(p_Y(.|{\bf x}), q_.({\bf x}))
\end{align}
$$


* Démontrer le théorème. 

On a la formule suivante:
$$
\mathbb{E}[L(q({\bf x}), y)|x] = h(p_Y(.|{\bf x}), q_.({\bf x}))

$$

avec $q_.({\bf x})$ la loi de Bernouilli de paramètre $q({\bf x})$. Or, on peut remarquer que $p_Y(.|x)$ suit la loi de Bernouilli de paramètre $q_{opt}$. Ainsi, on peut poser l'égalité suivante:

$$
\begin{align}
\mathbb{E}[L(q_{opt}({\bf x}), y)|{\bf x}] &= h(p_Y(.|x), p_Y(.|x))\\
&=h(p_Y(.|x))
\end{align}
$$

Or, on a montré l'inégalité suivante:$h(p, p') \geq h(p)$.

Ainsi, $h(p_Y(.|{\bf x}), q_.({\bf x})) \geq h(p_Y(.|x))$.

Donc,
\mathbb{E}(L(q_.({\bf x}, y))| {\bf x}) \geq \mathbb{E(L(q_{opt}({\bf x}, y))|{\bf x})}.\

Or, on sait que $p(x) \geq 0$.

Ainsi, en multipliant les deux membres de l'inégalité ci-dessus, et en intégrant sur le domaine de définition de x, on obtient le résultat voulu :
$$
\mathbb{E}(L(q({\bf x}), y))\geq  \mathbb{E}(L(q_{opt}({\bf x}), y))
$$

*Démontrer que le minimum de la fonction de perte correspond au maximum de la vraisemblance pour ce modèle. 


# Partie 2 : Reconnaissance de chiffres manuscrits (2 et 7)

## Lecture des données

* Lire les données MNIST. Il y a 4 ensembles de variables. 
```{r}
  library(keras)
  library(magrittr)
  mnist <- dataset_mnist()
  x_train <- mnist$train$x
  y_train <- mnist$train$y
  x_test <- mnist$test$x
  y_test <- mnist$test$y
```

* Filtrer les données correspondant aux chiffres $2$ et $7$. Commenter et compléter le code suivant. Exécuter le code en changeant l'option de chunck. 


```{r}
# On conserve les 2 et les 7 uniquement
  boo_train <- y_train == 2 | y_train == 7 
  x_train <- mnist$train$x[boo_train,,]
  y_train <- mnist$train$y[boo_train]

# Idem pour le set de test
  boo_test <- y_test == 2 | y_test == 7
  x_test <- mnist$test$x[boo_test,,]
  y_test <- mnist$test$y[boo_test]
```


* Utiliser la fonction `image()` pour visualiser le premier chiffre test de la base de données réduite. C'est un 7. 

```{r}
  image(t(x_test[1, 28:1,]), col = grey.colors(5))
```


* Les images de dimension 28x28 doivent être converties en vecteurs de longueur 784 ($= 28 \times 28$). Cela peut se faire de plusieurs manières. En particulier, la fonction `array_reshape()` de keras est très utile pour cela.  


```{r}
# reshape
  x_train <- array_reshape(x_train, c(nrow(x_train), 784))
  x_test <- array_reshape(x_test, c(nrow(x_test), 784))
```

* Normaliser les données pour obtenir des valeurs réelles (flottants) entre 0 et 1 en divisant les valeurs présentes par 255.  Utiliser la fonction `image()` pour visualiser le premier chiffre test de la base de données réduite dans cette nouvelle représentation. 


```{r}
# rescale
  x_train <- x_train/255
  x_test <- x_test/255

# le symbol %>% est similaire au 'pipe' d'unix (library(magrittr))
  dim(x_test)
  x_test[1,] %>% matrix(nrow = 28) %>% .[,28:1] %>% image(col = grey.colors(5))
```


* Les données de classe sont des entiers 2,7. Convertir ces données en variables booléennes ou binaires. La valeur 1 ou `TRUE` correspondra à un 7.
  
```{r}
  y_train <- y_train == 7
  y_test <-  y_test == 7
```


## Ajuster un réseau de neurone avec keras

* Construire un réseau de neurones à deux couches cachées à l'aide de la fonction `keras_model_sequential()`, en faisant varier les paramètres des couches comme ci-dessous. Définir les termes apparaissant dans la construction du modèle.


* Commenter et compléter le code suivant.  

```{r}
  model <- keras_model_sequential() 
  model %>% 
    # Initialisation de la première couche avec fonctoin d'activation "relu" et 256 neurones.
    layer_dense(units = 256, activation = 'relu', input_shape = 784) %>% 
    # On associe aléatoirement la valeur 0 à 50% des imahes d'entrée pour éviter l'over fitting.
    layer_dropout(rate = 0.5) %>% 
    # Initialisation de la deuxième couche avec 128 neurones.
    layer_dense(units = 128, activation = 'relu') %>%
    layer_dropout(rate = 0.5) %>%
    # Initialisation de dernière couche
    layer_dense(units = 1, activation = 'sigmoid')
```

* Donner un tableau de correspondance entre les notions mathématiques introduites dans la section précédente et les termes apparaissant dans la construction du modèle. 

```{r}
terme <- c("units = 256", "input_shape")
definition <- c("Dimension de la première couche cachée (W_2)", "Dimension des observations")
data.frame(terme, definition)
```


* Compiler le modèle en précisant la fonction de perte (binary) et demander de visualiser le taux de bonne classification (accuracy). Le choix de l'optimiseur est spécifié. 


```{r}
# Configuration du modèle pour l'entraînement  
  model %>% compile(
    loss = 'binary_crossentropy',
    optimizer = optimizer_rmsprop(lr = 0.001, rho = 0.9, decay = 0),
    metrics = c('accuracy')
)
```

* Donner un tableau de correspondance entre les notions mathématiques introduites dans la section précédente et les termes apparaissant dans la compilation du modèle. 

```{r}
terme <- c("loss", "optimizer", "lr", "rho", "decay", "accuracy")
definition <- c("Fonction de perte","Méthode de gradient stochastique","Taux d'apprentissage (pas dans la descente de gradient)","Facteur de décroissance moyenne du gradient","Baisse du taux d'apprentissage à chaque itération","Précision utilisée pour mesurer la pertinence du modèle")
data.frame(terme, definition)
```



* Allons-y Alonso, pour ajuster le réseau (apprentissage)

```{r}
# A pprentissage : on entraîne le modèle
  history <- model %>% fit(
                        x_train, 
                        y_train, 
                        epochs = 20, 
                        batch_size = 128, 
                        validation_data = list(x_test, y_test)
)
```

* Donner un tableau de correspondance entre les notions mathématiques introduites dans la section précédente et les termes apparaissant dans l'apprentissage du modèle. 
```{r}
terme <- c("x_train", "y_train", "epochs", "batch_size", "validation_data")
definition <- c("Ensemble des images (observations)","Sortie attendue","Nombre d'entraînements du modèle", "Nombre d'images utilisées par le réseau à chaque itération", "Données permettant d'évaluer le modèle")
data.frame(terme, definition)
```
* Quelle est signification des courbes `loss` et `val_loss` que l'on voit tracées? 

```{r}
plot(history)
```

Les coubes "loss" et "val_loss" représentent respectivement les pertes du modèle pour les données d'entraînement (x_train, y_train) et pour les données de tests (x_test, y_test).

* Evaluer le modèle sur les données de test (erreur de classification et perte log loss)


```{r}
model %>% evaluate(x_test, y_test)
```


* Donner une matrice de confusion pour les classements effectués par le modèle sur le jeu test

```{r}
# help(table)
pred_class <- model %>% predict_classes(x_test)
table(predicted = pred_class, observed = mnist$test$y[boo_test])
```

* Montrer deux chiffres manuscrits que la machine n'a pas réussi à classer correctement. Donner la liste des mal classés et les probabilités de classement pour chacune des erreurs.

```{r}
index = 1:2060
wrong = index[pred_class[, 1]!=y_test]

# On choisit d'afficher les images d'index 145 et 223, que l'on récupère depuis la liste wrong qui représente la liste des index mal classés.
x_test[145,] %>% matrix(nrow = 28) %>% .[,28:1] %>% image(col = grey.colors(5))
x_test[223,] %>% matrix(nrow = 28) %>% .[,28:1] %>% image(col = grey.colors(5))

# On affiche la liste des index mal classés ainsi que leurs probabilités de classement
pred <- predict_proba(model, x_test)[wrong]
data.frame(wrong, pred)
```

## Défi MNIST 

Pour les données de l'exercice précédent, répondre aux questions suivantes.
  
  Construisons 6 modèles dont nous feons varier les paramètres de la anière suivante:
  - nombre de couches cachées : 1 ou 5, 
  - nombre de neurones par couche cachée : 10 ou 100, 
  - valeur de `dropout` par couche cachée : 0.2 ou 0.8.
  
```{r}
set.seed(3)
nb_couches <-c(1, 5)
nb_neurones<-c(10, 100)
val_dropout<-c(0.2, 0.8)



create_model = function (nb_couches, nb_neurones, val_dropout){
  #Renvoie un modèle correspondant aux critères entrés en paramètres
  wanted_model <- keras_model_sequential()
  wanted_model %>% 
    # Initialisation de la première couche avec fonction d'activation "relu" et 256 neurones.
    layer_dense(units = nb_neurones, activation = 'relu', input_shape = 784) %>% 
    layer_dropout(rate = val_dropout)
  if(nb_couches != 1){
    for (i in 1:nb_couches){
      wanted_model %>% 
      # Initialisation des couches suivantes
      layer_dense(units = nb_neurones, activation = 'relu') %>% 
      layer_dropout(rate = val_dropout)
    }
  }
  wanted_model %>% 
  # Initialisation de dernière couche
  layer_dense(units = 1, activation = 'sigmoid')
  return (wanted_model);
}


modele_1_10_2 <- create_model(1, 10, 2)
modele_1_10_8 <- create_model(1, 10, 8)
modele_1_100_2 <- create_model(1, 100, 2)
modele_1_100_8 <- create_model(1, 100, 8)
modele_5_10_2 <-  create_model(5, 10, 2)
modele_5_10_8 <-  create_model(5, 10, 8)
modele_5_100_2 <-  create_model(5,100, 2)
modele_5_100_8 <- create_model(5, 100, 8)

res_log_loss<-c()
res_acc<-c()
compteur = 1

model_instructions = function(model1){

      # Configuration du modèle pour l'apprentissage
      model1 %>% compile(
          loss = 'binary_crossentropy',
          optimizer = optimizer_rmsprop(lr = 0.001, rho = 0.9, decay = 0),
          metrics = c('accuracy')
      )
      
      # Entraînement du réseau
      history <- model1 %>% fit(
                              x_train, 
                              y_train, 
                              epochs = 20, 
                              batch_size = 128, 
                              validation_data = list(x_test, y_test))
      
      res <- c()
      res = model1 %>% evaluate(x_test, y_test)
      return(res)

}

res_log_loss[1] = model_instructions(modele_1_10_2)$loss
res_log_loss[2] = model_instructions(modele_1_10_8)$loss
res_log_loss[3] = model_instructions(modele_1_100_2)$loss
res_log_loss[4] = model_instructions(modele_1_100_8)$loss
res_log_loss[5] = model_instructions(modele_5_10_2)$loss
res_log_loss[6] = model_instructions(modele_5_10_8)$loss
res_log_loss[7] = model_instructions(modele_5_100_2)$loss
res_log_loss[8] = model_instructions(modele_5_100_8)$loss
  
res_acc[1] = model_instructions(modele_1_10_2)$acc
res_acc[2] = model_instructions(modele_1_10_8)$acc
res_acc[3] = model_instructions(modele_1_100_2)$acc
res_acc[4] = model_instructions(modele_1_100_8)$acc
res_acc[5] = model_instructions(modele_5_10_2)$acc
res_acc[6] = model_instructions(modele_5_10_8)$acc
res_acc[7] = model_instructions(modele_5_100_2)$acc
res_acc[8] = model_instructions(modele_5_100_8)$acc

names<-c("modele_1_10_2", "modele_1_10_8", "modele_1_100_2", "modele_1_100_8", "modele_5_10_2", "modele_5_10_8", "modele_5_100_2","modele_5_100_8")
data <- data.frame(names, res_log_loss, res_acc)
barplot(res_log_loss, main = "Comparaison des erreurs de logloss", ylab = "logloss", legend.text = data$names,  args.legend = list(x = "topleft"), col = rainbow((8)))
barplot(res_acc, main = "Comparaison des erreurs de classification", ylab = "accuracy", ylim = c(0.96, 0.99), legend.text = data$names, args.legend = list(x = "bottomleft"), col = rainbow(8))
```


* Quel modèle de prédiction vous parait être le meilleur ? 

Afin de faire notre choix, on essaie de trouver un modèle qui maximise l'accuracy et qui minimise la valeur du log loss, ainsi on choisi le modèle avec les paramètres 1, 100, 0.2.

* Donner la liste des chiffres mal classés et les probabilités de classement pour chacune des erreurs. 
```{r}
probas<-c()
pred_class <- modele_1_100_2 %>% predict_classes(x_test)
wrong = index[pred_class[, 1]!=y_test]
for (i in 0:length(wrong)){
  probas[i] = predict_proba(modele_1_100_2, x_test)[wrong[i]]
}
data.frame(wrong, probas)
```

# Partie 3 : Analyse de critiques de films (IMDB)



```{r}
library(magrittr)
library(keras)

```

## Index des termes et sacs de mots.


```{r}
  index <- keras::dataset_imdb_word_index()
  names(index[index == 1])
```

Sans surprise, le terme le plus fréquemment utilisé est l'article "the". Sa valeur dans l'index est donc égale à 1. 


```{r}
  # Réordonnons l'index
  o <- as.numeric(index) %>% order()
  index[o[1:10]] %>% names()
```

Nous voyons qu'il s'agit d'articles, de prépositions ou de termes non-informatifs, comme par exemple, des éléments extraits des balises html (br). Il sera peut-être préférable d'éliminer les termes les plus utilisés. L'entrée 49 correspondant au terme "good".

Pour réduire le temps de calcul, nous conservons uniquement les 5000 termes les plus fréquents.

```{r}
  imbd <-  keras::dataset_imdb(path = "imdb.npz",
                              num_words = 5048, # index d dernier terme pris en compte
                              skip_top = 49, # élimination des premiers termes (non pertinents)
                              oov_char = -2, # craractère qui remplace les termes éliminés
                              start_char = -1, # début de séquence
                              index_from = 0)
```


## Défi "analyse de sentiments"

### Lecture des données

* Ecrire une ligne de commande R permettant convertir le document 12 en un vecteur de longueur 5000, indiquant le nombre d'apparition de chacun des indices allant de 1 à 5000 dans ce document.  


```{r}
  # Conversion du document 12 en un vecteur de longueur 5000
  help(sapply)
  doc <- imbd$train$x[[12]]
  result <- sapply(49:5048, FUN = function(x) length(doc[doc == x]))
  result[1:100]
```

* Constituer un jeu de données comportant 10000 documents choisis pour moitié dans l'ensemble "train" et pour moitié dans l'ensemble "test" de l'IMBD. Techniquement nous le constituerons en 20 étapes, pour limiter l'impact sur la mémoire. Commenter et exécuter le code suivant


```{r}
x_imbd <- NULL

  for (i in 1:10){
    
    x_imbd_500 <- NULL
    
      for (j in (500*(i-1)+1):(500*i)){
        
        # on récupère les 5000 premiers documents de imbd$train
        
        doc_temp <- imbd$train$x[[j]]
        x_imbd_500 <- rbind(x_imbd_500, 
                         sapply(49:5048, 
                                FUN = function(ind) sum(doc_temp == ind)))
        
        #if (j%%500 == 0) print(j) # ca rassure
    }
    x_imbd <- rbind(x_imbd, x_imbd_500)
  }

 for (i in 1:10){
   
    x_imbd_500 <- NULL
    
      for (j in (500*(i-1)+1):(500*i)){
        
        # On récupère les 5000 premiers documents de x$test
        
        doc_temp <- imbd$test$x[[j]]
        x_imbd_500 <- rbind(x_imbd_500, 
                         sapply(49:5048, 
                                FUN = function(ind) sum(doc_temp == ind)))
        
        #if (j%%500 == 0) print(j) # ca rassure
    }
    x_imbd <- rbind(x_imbd, x_imbd_500)
  }
```

* Que contient l'objet `x_imbd` ?\
x_imbd contient désormais 10000 documents. Les 5000 premiers appartenaient à l'ensemble "train", les autres à l'ensemble "test". Les documents sont stockés sous la forme de vecteurs de longueurs 5000. A la position d'indice i, on trouve la fréquence d'apparition du terme d'index i dans le document. 

* Définisson les classes $y = 0$ ou $y=1$ pour chaque élément de `x_imbd`
```{r}
classe <- c("y=0", "y=1")
definition <- c("opinion positive", "opinion négative")
data.frame(classe, definition)
```


```{r}
# Récupération de s valeurs y associées aux fichiers sélectionnés                                           
  y_imbd <- c(imbd$train$y[1:5000], imbd$test$y[1:5000])
```

Et voilà. On est en pleine forme et on dispose d'une base d'apprentissage comportant les fréquences d'apparition des mots de l'index pour 10000 documents (`x_imbd`) et les opinions des utilisateurs `y_imbd`. Le défi peut vraiment commencer.


#### Etude d'association

Le but d'une étude d'association est d'identifier les termes les plus associés aux opinions positives ou négatives des utilisateurs. Pour cela, nous evaluons la correlation au carré entre l'occurrence de chaque terme et l'opinion de l'utilisateur (présence d'un 1). Il se peut que certaines valeurs de corrélation ne soient pas calculables à cause d'un écart-type nul. 

* Calcul du coefficient de corrélation au carré entre les fréquences d'apparition des termes de l'index et opinion des utilsateurs (5000 valeurs).

```{r}

  # termes
  x <- x_imbd
  
  # opinion
  y <- y_imbd
  
  # comment
  r <- cor(x, y)
  r2 <- r^2

```


* Montrer les termes dont la valeur d'association $r^2$ est supérieure à 3 pour cent (0.02), puis supérieure à 0.02, et à 0.01. _Note_ : Il faut effectuer un décalage de 48 indices dans l'index pour trouver le codage correct.


```{r}
  # quelque chose à changer
  index[o[ which(r2 > 0.03) + 48 ]] %>% names()
  index[o[ which(r2 > 0.02) + 48 ]] %>% names()
  index[o[ which(r2 > 0.01) + 48 ]] %>% names()
```


On remarque que plus $r^2$ diminue, moins les mots permettent de se faire une idée sur l'opinion du commentateur. Par exemple, lorsque $r^2 > 0.03$, on trouve le terme "bad" qui nous indique que l'opinion est négative. Au contraire, lorsque $r^2 > 0.01$, on a par exemple les mots "life" ou "acting" qui ne permettent pas de classifier le commentaire.


* Dans quelles proportions les termes de valeur d'association $r^2$ supérieure à 0.02 apparaissent-ils dans les documents ? Représenter graphiquement ces proportions à l'aide d'un diagramme en barre. 


```{r}

title = " Fréquence  des termes tels que r² > 0.02"
# Calculer la frequence des termes realisant la condition
diag_freq = function(corr, color, title){
  ind <- c()
  list <- index[o[ which(r2 > corr) + 48 ]]
  for (i in 1:length(list)){
    ind[i]=list[[i]]
  }

  freq <- x[,ind] %>% apply(2, mean) #moyenne sur les colonnes
# mots dans l'index et barplot
  names(freq) <-  index[o[which(r2 > corr) + 48]] %>% names() 
  b = barplot(sort(freq, decreasing = TRUE), col = color, las = 2, main = title)
  return(freq)
}


diag_freq(0.02, "lightblue", title)
```



* Dans quelles proportions les termes de valeur d'association $r^2$ supérieure à 0.01 apparaissent-ils dans les documents ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre.

```{r}
title = " Fréquence  des termes tels que r² > 0.01"
diag_freq(0.01, "lightblue", title)
```



* Dans quelles proportions les termes de valeur d'association $r = cor(x,y)$ supérieure à 0.1 apparaissent-ils dans les documents ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre.

```{r}
ind <- c()
  list <- index[o[ which(r > 0.1) + 48 ]]
  for (i in 1:length(list)){
    ind[i]=list[[i]]
  }

  freq <- x[,ind] %>% apply(2, mean) #moyenne sur les colonnes
# mots dans l'index et barplot
  names(freq) <-  index[o[which(r > 0.1) + 48]] %>% names() 
  b = barplot(sort(freq, decreasing = TRUE), col = "pink", las = 2, main = " Fréquence  des termes tels que r > 0.1")

```



*  Dans quelles proportions les termes de valeur d'association $r$ inférieure à $-0.1$ apparaissent-ils dans les documents ? Représenter graphiquement ces proportions  à l'aide d'un diagramme en barre.


```{r}

ind <- c()
list <- index[o[ which(r < -0.1) + 48 ]]
  for (i in 1:length(list)){
    ind[i]=list[[i]]
  }
  
freq <- x[,ind] %>% apply(2, mean) #moyenne sur les colonnes
# mots dans l'index et barplot
names(freq) <-  index[o[which(r < -0.1) + 48]] %>% names()  
barplot(sort(freq, decreasing = TRUE), col = "palegreen", las = 2, main = " Fréquence  des termes tels que r < - 0.1")
```

Apriori, plus on a de termes dont la corélation avec y est positive, plus le commentaire a de chances d'être négatif. Or on observe ici des termes à connotations négatives dont la corélation avec y est négative, et inversement pour les termes positifs, ce qui étonne nos scientifiques, y compris les plus doués.

#### Modèles d'apprentissage 

Voici le code R correspondant au modèle qu'on a choisi: 


```{r}
set.seed(seed = 5427381)
x_train <- x[1:5000,]
x_test <- x[5001:10000,]
y_train <- y[1:5000]
y_test <- y[5001:10000]

val_dropout = 0.5
nb_neurones = 10
nb_couches = 10
# code de mon meilleur modèle
model<-keras_model_sequential()

# Initialisation de la première couche avec fonction d'activation "relu" et 256 neurones.
model %>%
layer_dense(units = nb_neurones, activation = 'relu', input_shape = 5000) %>% 
layer_dropout(rate = val_dropout) %>%
  
# Initialisation des couches suivantes
for(i in 1: nb_couches){
  model %>%
  layer_dense(units = nb_neurones, activation = 'relu') %>% 
  layer_dropout(rate = val_dropout)
}  

# Initialisation de ladernière couche
model %>%
layer_dense(units = 1, activation = 'sigmoid')


# Configuration du modèle pour l'apprentissage
model %>% compile(
          loss = 'binary_crossentropy',
          optimizer = optimizer_rmsprop(lr = 0.001, rho = 0.9, decay = 0),
          metrics = c('accuracy')
      )

# Entraînement du réseau
history <- model %>% fit(
                          x_train, 
                          y_train, 
                          epochs = 20, 
                          batch_size = 500, 
                          validation_data = list(x_test, y_test))

# Evaluation du modèle
model %>% evaluate(x_test, y_test)
```



```{r echo = FALSE}
dropout_val <- c(0.2, 0.4, 0.4, 0.5, 0.5 , 0.5)
nb_neurone<-c(100, 10, 4, 10, 1000, 1000)
nb_couche <-c(3, 5, 10, 10, 1000, 1000)
optimizer_type <-c("rmsprop", "adam", "rmsprop", "rmsprop", "rmsprop", "rmsprop")
nb_epochs <-c(20, 20, 200, 20, 25, 30)
logloss_val <-c(1.1, 0.52, 1.43, 0.337, 0.352, 0.362)
accuracy_val<-c(0.84, 0.86, 0.81, 0.86, 0.86, 0.86)
batchsize <-c(128, 128, 128, 500, 4000, 4000)
data.frame(optimizer_type, dropout_val, nb_neurone, nb_couche, nb_epochs, batchsize, logloss_val, accuracy_val)
```


Ainsi après plusieurs jours d'experiences, nos scientifiques ont décidé de garder la combinaison suivante: 

| Drouptout    |   optimizer   |    neurones    |     couches | epochs |  batch size | 
|------------ |:--------------:|---------------|:------------:| ------:| -----------:|
|0.5          | rmsprop        |    10          |        10   |  20    |   500       |

